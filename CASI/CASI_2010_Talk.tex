
\documentclass[a4]{beamer}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{subfigure}
\usepackage{newlfont}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{beamerthemesplit}
\usepackage{pgf}%,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{natbib}
\bibliographystyle{chicago}
\mode<presentation> {
\usetheme{Madrid}
\useoutertheme{shadow}
\usecolortheme{whale}
\usecolortheme{orchid}
}

\setbeamercovered{dynamic}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
%\usepackage{times}
\usepackage[T1]{fontenc}

\usepackage{beamertexpower}

\title[paired-samples $t$-test]{Testing Paired Data}

%\title[Paired Samples]{On the fallacy of Bland-Altman method comparison plots}


\author[Kevin Hayes]{Kevin Hayes, Anthony Kinsella,\\ and Kevin O'Brien}
\institute[UL]{Department of Mathematics and Statistics,\\ University \textit{of} Limerick.}

\date[CASI 2010]{The 2010 Conference of Applied\\ Statistics in Ireland, May 16\textit{th} to 18\textit{th}.}


\begin{document}

\frame{\titlepage}


\section<presentation>*{Outline}

\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections,hideallsubsections]
\end{frame}

\AtBeginSection[] {
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[current,currentsection,hideallsubsections]
  \end{frame}
}


\section{A Look at Some Tests for Paired-Samples}

\begin{frame}[b]
 Let $Y_1$ and $Y_2$ be distributed bivariate normal:\ $\mathrm{E}(Y_1)=\mu_1$,\\ $\mathrm{E}(Y_2)=\mu_2$,
 $\mathrm{var}(Y_1)=\sigma^2_1$, $\mathrm{var}(Y_2)=\sigma^2_2$, and $-1<\rho_{12}<1$.

 \bigskip

 \uncover<2->{Of interest are the tests of the hypotheses:}
 \begin{enumerate}
   \item<2-| alert@7> H$'$: $\mu_1-\mu_2=0$,\ \ \ \cite{Student};
   \item<2-| alert@8> H$''$: $\sigma^2_1 =\sigma^2_2$,\ \ \cite{Pit39}, \cite{Morgan39};
   \item<2-| alert@9> H$^{\textrm{J}}$: $\mu_1-\mu_2=0$ \textbf{and} $\sigma^2_1=\sigma^2_2$,\ \ \cite{BradBlack89}.
 \end{enumerate}

 \bigskip

 \uncover<3->{Define $D=Y_1-Y_2$ and $S=Y_1+Y_2$ } \pause\pause
        \parstepwise
       {
        \begin{eqnarray*}
          \step{\alert<7,9>{ \mathrm{E}(D)} }& \restep{\alert<7,9>{=}}&
          \restep{\alert<7,9>{ \mu_D\ =\ \mu_1-\mu_2}}\\
          \restep{\mathrm{E}(S)} & \restep{=} &
          \restep{\mu_S\ =\ \mu_1+\mu_2,}\\
          \step{ \mathrm{var}(D)} & \restep{=} &
          \restep{ \sigma^2_D\ =\ \sigma_1^2+ \sigma_2^2 -2\rho_{12}\sigma_1 \sigma_2, } \\
          \restep{\mathrm{var}(S)} & \restep{=} &
          \restep{\sigma^2_S\ =\ \sigma_1^2+ \sigma_2^2
          +2\rho_{12}\sigma_1 \sigma_2,} \\
          \step{\alert<8,9>{\mathrm{cov}(D,S)}} & \restep{\alert<8,9>{=}} &
          \restep{\alert<8,9>{\sigma_1^2 - \sigma_2^2.}}
        \end{eqnarray*}
       }
\end{frame}


\begin{frame}
 \begin{exampleblock}{H$'$: $\mu_1-\mu_2=0$,\ \ \ \cite{Student}} The test of the hypothesis H$'$: $\mu_D=0$ due to \cite{Student} is based on the \emph{t}-ratio
  \begin{equation}
   \label{StudentT} T^\ast = \frac{ \bar{D} }{ \hat{\sigma}_D/ \sqrt{n}} \sim t_{(n-1)\textrm{df}},
  \end{equation}
 where $\bar{D}$ is the mean of the $n$ case-wise differences $D_i=Y_{i1}-Y_{i2}$ of the pairs $(Y_{i1},Y_{i2}),\ i=1,\ldots,n$ and
  \[
   \hat{\sigma}^2_D = \frac{1}{ n-1} \sum_{i=1}^n (D_i-\bar{D})^2
  \]
 is the usual estimate of the variance of the differences $\sigma^2_D.$
 \end{exampleblock}

\begin{center}
\uncover<2->{ \alert{This classical test makes no assumptions about the equality\\ (or otherwise) of the variance parameters $\sigma^2_1$ and $\sigma^2_2.$}}
\end{center}
\end{frame}


\begin{frame}
\begin{exampleblock}{ H$''$: $\sigma^2_1 =\sigma^2_2$,\ \ \cite{Pit39}, \cite{Morgan39}} The test of the hypothesis that the variances $\sigma^2_1$ and $\sigma^2_2$ are equal, which was devised concurrently by \cite{Pit39} and \cite{Morgan39}, is based on the correlation of $D$ with $S,$ the coefficient being $\rho_{DS} = (\sigma^2_1 -\sigma^2_2) / ( \sigma_D \sigma_S ),$ which is zero if, and only if, $\sigma^2_1 = \sigma^2_2.$ Consequently a test of $\textrm{H}''\textrm{:}\ \sigma^2_1 = \sigma^2_2$ is equivalent to a test of $\textrm{H}''\textrm{:}\ \rho_{DS}=0$ and the test statistic is the familiar {\it t}-test for a correlation coefficient with $(n-2)$ degrees of freedom:
\begin{equation}\label{tPitMorg}
T^*_\mathrm{PM} = R \sqrt{ \frac{n-2}{1-R^2} },
\end{equation}
where
\begin{equation}\label{Rcoeff}
R = \frac{ \sum (D_i-\bar{D})(S_i-\bar{S}) }{ [ \sum(D_i-\bar{D})^2 \sum (S_i-\bar{S})^2 ]^{\frac{1}{2}} }
\end{equation}
is the sample correlation coefficient of the $n$ case-wise differences $D_i$ and sums $S_i = Y_{i1} + Y_{i2}.$
\end{exampleblock}
\end{frame}


\begin{frame}
 \begin{exampleblock}{H$^{\textrm{J}}$: $\mu_1-\mu_2=0$ \textbf{and} $\sigma^2_1=\sigma^2_2$,\ \ \cite{BradBlack89}}
 They write the conditional expectation of $D$ given $S$ as
  \begin{eqnarray*}
    \mathrm{E}(D|S) & = & \mu_D + \mathrm{cov}(D,S)
        \mathrm{var}(S)^{-1}(S-\mu_S), \nonumber \\
        & = & \mu_D + [(\sigma^2_1-\sigma^2_2)/ \sigma^2_S]
        (S - \mu_S),\\
        & = &\beta_0 + \beta_1 S,
  \end{eqnarray*}
  where\ \ $\beta_0=\mu_D-[(\sigma^2_1-\sigma^2_2)/ \sigma^2_S] \mu_S$\ \ and\ \ $\beta_1 = (\sigma^2_1-\sigma^2_2)/ \sigma^2_S.$\pause\\ \bigskip

  The hypothesis H$^{\textrm{J}}$: $\mu_D = 0$ and $\sigma^2_1 = \sigma^2_2,$ is true \ \textit{iff} \ $\beta_0=\beta_1=0.$\pause\\ \bigskip

  The test statistic is
\begin{equation}\label{BB:Fstat}
F^* = (\frac{n-2}{2}) (\frac{\sum {D_i^2} - \mathrm{SSE}}{\mathrm{SSE}}) \sim F_{(2,n-2)\textrm{df}} ,
\end{equation}
where $\mathrm{SSE}$ is the residual error sum-of-squares from \alert<4->{$\hat{D}_i=\hat{\beta}_0 +\hat{\beta}_1 s_i$.}
 \end{exampleblock}
\end{frame}



\section{An Alternative Paired-Sampes $t$-Test}

\begin{frame}[t]
Write
\begin{eqnarray*}
\mu_{D \mid S=s} &=& \mu_D + [ ( \sigma^2_1 - \sigma^2_2) / \sigma^2_S ] (s - \mu_S)\\
 &=& \beta_0 + \beta_1 s
 \end{eqnarray*}
where $\beta_1 = (\sigma^2_1 - \sigma^2_2 )/ \sigma^2_S$ and $\beta_0 = \mu_D- \beta_1 \mu_S$ with $\sigma^2_{D\mid S} = \sigma^2_D - ( \sigma^2_1 - \sigma^2_2 )^2 / \sigma^2_S.$\pause
\medskip

Can also be written $\structure<2>{D_i = \beta_0 + \beta_1 s_i + \varepsilon_i}\ (i=1,\ldots,n)$ where $\varepsilon_i$ are IID $\mathrm{N}(0,\sigma^2_\varepsilon)$ distribution having $\sigma^2_\varepsilon = \sigma^2_{D|S},$ and the $s_i$ terms are the values in the conditioning set $S_1 = s_1, S_2 = s_2, \ldots , S_n=s_n$ and are restricted only by the exclusion of the trivial case $s_1 = s_2 = \cdots = s_n.$ \medskip\pause

Can be reformulated as $\structure<3>{D_i = \alpha + \beta_1 ( s_i - \bar{s} ) + \varepsilon_i}$ where the intercept parameters are related by $\alpha = \mu_D + \beta_1 (\bar{s}-\mu_S),$ and where the slope parameters and the error structures of the two regression models are identical.
\end{frame}

\begin{frame}
\alert<1>{Treating the $s_i$ terms as fixed known constants} the MLEs are:
\begin{eqnarray*}
\hat\alpha &=& \bar{D},\\ \\
\hat\beta_1 &=& \frac{ \sum (D_i-\bar{D})(s_i-\bar{s}) }{ \sum (s_i-\bar{s})^2 }\\
 &=& \frac{ \sum D_i (s_i-\bar{s}) }{ \sum (s_i-\bar{s})^2},\\ \\
\hat\sigma^2_\varepsilon &=& \frac{1}{n} (\ \sum(D_i-\bar{D})^2 - \frac{[\sum(s_i-\bar{s})D_i]^2  }{ \sum(s_i-\bar{s})^2}\ )
\end{eqnarray*}\medskip \pause\pause

The estimates $\hat\alpha$ and $\hat\beta_1$ are independent and have distributions $\mathrm{N}(\alpha, n^{-1}\sigma^2_\varepsilon)$ and $\mathrm{N}(\beta_1, [\sum(s_i-\bar{s})^2]^{-1}\sigma^2_\varepsilon).$
\end{frame}

\begin{frame}
The random variables
\begin{eqnarray*}
Q_0 &=& n(\bar{D}-\alpha)^2 / \sigma^2_\varepsilon\ \sim\ \chi^2_{1\mathrm{df}},\\ \\
Q_1 &=& (\hat\beta_1-\beta_1)^2 \sum(s_i-\bar{s})^2 / \sigma^2_\varepsilon\ \sim\ \chi^2_{1\mathrm{df}}\\ \\
Q_2 &=& n\hat\sigma^2_\varepsilon / \sigma^2_\varepsilon\sim \chi^2_{(n-2)\mathrm{df}},
\end{eqnarray*}
are independent and
\[
Q=\sum [ D_i - \alpha - \beta(s_i - \bar{s}) ]^2 \sim \chi^2_{n\mathrm{df}}.
\]
\end{frame}

\begin{frame}
Writing $\tilde\sigma^2_\varepsilon = n \hat\sigma^2_\varepsilon / (n-2),$ there are, consequently, two independent $F$ ratio tests from the full model:
\[
F_0^\ast = \frac{n\bar{D}^2}{\tilde\sigma^2_\varepsilon} \sim F_{(1,n-2)\mathrm{df}} ,
\]
testing $\textrm{H}\colon\ \alpha=0$ in favour of a reduced model with no intercept parameter; and
\[
F_1^\ast = \frac{ \hat\beta_1^2 \sum\limits_{i=1}^{n}(s_i-\bar{s})^2 }{ \tilde\sigma^2_\varepsilon } \sim F_{(1,n-2)\mathrm{df}} ,
\]
testing $\textrm{H}\colon\ \beta_1=0$ in favour of a model with no slope parameter.
\end{frame}


\begin{frame}
Since $(T_{k\textrm{df}})^2 \equiv F_{(1,k)\textrm{df}}$ these $F$ ratio tests can be reexpressed as two Student \textit{t}-tests, each with $(n-2)$ degrees of freedom:
\begin{equation}
\label{tZero}
T_0^\ast = \frac{ \bar{D} }{ \tilde{\sigma}_\varepsilon / \sqrt{n}} \sim t_{(n-2)\textrm{df}}
\end{equation}
and
\begin{equation}
\label{tOne}
T_1^\ast = \frac{ \hat\beta_1 }{ \tilde{\sigma}_\varepsilon / \sqrt{\sum{(s_i-\bar{s})^2}}} \sim t_{(n-2)\textrm{df}}.
\end{equation}
\end{frame}

\begin{frame}
As $Q_0$ and $Q_1$ are independent it also follows that
\[
\frac{1}{2}(Q_0 + Q_1) \sim \chi^2_{2\mathrm{df}},
\]
and a third $F$-ratio testing $\textrm{H}\colon \alpha = \beta_1 = 0$ in favour of a model with no intercept parameter and no slope parameter can be formed as
\begin{equation} \label{BBHayes}
F^\ast = \frac{ (T_0^\ast)^2 + (T_1^\ast)^2 }{2} \sim F_{(2,n-2)\textrm{df}} ,
\end{equation}
and is an alternative representation of the Bradley-Blackwood test statistic given in (\ref{BB:Fstat}) above.
\end{frame}

\section{Power Functions}

\begin{frame}
\begin{itemize}
\item The power function for the test of the marginal hypothesis H: $\mu_D=0$ using Student's paired {\it t}-test is characterized by a non-central {\it F}-distribution with degrees of freedom parameters 1 and $(n-1)$ in the numerator and denominator, and non-centrality parameter $\delta = n(\mu_D)^2 / \sigma^2_D.$
\item<2-> The power function for the test of the marginal hypothesis H: $\mu_D=0$ using the t-ratio $T_0^\ast$ is a non-central {\it
F}-distribution with degrees of freedom parameters 1 and $(n-2)$ in the numerator and denominator, and non-centrality parameter $\delta_0 = n(\mu_D)^2 / \sigma^2_{D|S},$ where $\sigma^2_{D|S} = \sigma^2_D - (\sigma^2_1-\sigma^2_2 )^2/ \sigma^2_S=\sigma^2_D(1-\rho_{DS}^2).$
\item<3-> These results follow as a special cases of power functions for tests of the general linear hypothesis in univariate linear models \citep{OBrienMuller}.
\end{itemize}
\end{frame}

%\begin{frame}
%  \begin{figure}[!ht]
%      \begin{center}
%      \includegraphics[width=70mm]{Fig1PowerN10.pdf}
%      \end{center}
%  \end{figure}
% \end{frame}

%\begin{frame}
%  \begin{figure}[!ht]
%      \begin{center}
%      \includegraphics[width=70mm]{Fig2PowerN30.pdf}
%      \end{center}
%  \end{figure}
% \end{frame}



\section{Concluding Remarks}

\begin{frame}
\begin{itemize}
\item A closed test procedure based on the maximum and minimum t-ratios.
\item[]
\item Hsu's paper ``On samples from a normal bivariate population" discusses 7 tests of hypotheses relevant to this discussion.
\item[]
\item The Student t-test is UMP.
\item[]
\item Question the extent to which the results depend on Normality?
\item[]
\item Model selection and Bayes factors.
\item[]
\item \texttt{kevin.hayes@ul.ie}
\end{itemize}
\end{frame}



%\begin{frame}
%\frametitle{Bibliography}
%    \scriptsize
%    \begin{thebibliography}{}
%%
%\bibitem[Altman(1999)]{Altman99}
%Altman, D.~G. (1999).
%\newblock {\em Practical Statistics for Medical Research}.
%\newblock Chapman \& Hall Ltd.
%%
%
%\bibitem[Altman and Bland(1983)]{AB83}
%Altman, D.~G. and J.~M. Bland (1983).
%\newblock Measurement in medicine: {T}he analysis of method comparison studies.
%\newblock {\em The Statistician: Journal of the Institute of Statisticians\/}~{\em 32}, 307--317.
%
%\bibitem[Bland and Altman(1986)]{BA86}
%Bland, J.~M. and D.~G. Altman (1986).
%\newblock Statistical methods for assessing agreement between two methods of clinical measurement.
%\newblock {\em The Lancet, i\/}.
%
%\bibitem[Hollis(1996)]{Hollis96}
%Hollis, S. (1996).
%\newblock Annals of clinical biochemistry.
%\newblock {\em Annals of Clinical Biochemistry\/}~{\em 33}, 1--4.
%
%\bibitem[Ryan and Woodall(2005)]{Ryan:Wood:2005}
%Ryan, T. and W.~Woodall (2005).
%\newblock The most-cited statistical papers.
%\newblock {\em Journal of Applied Statistics\/}~{\em 32\/}(5), 461--474.
%
%\bibitem[Bland and Altman(1999)]{BA99} Bland J.M., Altman D.G. Measuring agreement in method comparison studies. \textit{Statistical Methods in Medical Research} 1999; \textbf{8}:135--160.
%%
%\bibitem[Bland and Altman(1995)]{BA95} Bland J.M., Altman D.G. Comparing methods of measurement:
%why plotting difference against standard method is misleading.
%\emph{Lancet} 1995; \textbf{346}:1085--1087.
%%
%\bibitem[Kinsella(1986)]{Kins86} Kinsella A. Estimating method precisions. \textit{The Statistician, Journal of the Royal Statistical Society Series D} 1986; \textbf{35}(4):421--427.
%
%\bibitem[Hogg et al.(2005)]{Hogg:2005}
%    Hogg, R V. and McKean, J W. and Craig, A T.,
%    Introduction to Mathematical Statistics,
%    6th edition, Pearson.
%
%
%\bibitem[Bartko, 1994]{Bartko94}
%    Bartko, J.~J.\ (1994).
%    Measures of Agreement: A Single Procedure.
%    \emph{Statistics in Medicine}, {\bf{13}}, 737--745.
%%
%\bibitem[Bradley and Blackwood, 1989]{BradBlack89}
%    Bradley, E.~L.\ and Blackwood, L.~G.\ (1989).
%    Comparing Paired Data: a Simultaneous Test for Means and Variances.
%    \emph{The American Statistician}, {\bf{43}}, 234--235.
%%
%\bibitem[Morgan, 1939]{Morgan39}
%    Morgan, W.~A.\ (1939). A Test for the Significance of the Difference Between Two Variances in a Sample from a Normal Bivariate Population.
%    \emph{Biometrika}, {\bf{31}}, 13--19.
%%
%
%\bibitem[O'Brien and Muller, 1993]{OBrienMuller}
%    O'Brien, R.~G. and Muller, K.~E.\ (1993).
%    ``Unified Power Analysis for t-tests Through Multivariate Hypotheses,"
%     In: Edwards, L.~K.\ {\em Applied Analysis of Variance in the Behavioral Sciences},
%    New York: Marcel Dekker.
%%
%\bibitem[Pitman, 1939]{Pit39}
%    Pitman, E.~J.~G.\ (1939). A Note on Normal Correlation.
%    \emph{Biometrika}, {\bf{31}}, 9--12.
%%
%\bibitem[Student, 1908]{Student}
%    [``Student"] Gosset, W.~S.\ (1908). The Probable Error of the Mean.
%    {\em Biometrika}, {\bf{6}}, 1--25.
%%
%    \end{thebibliography}
%\end{frame}

\end{document}
